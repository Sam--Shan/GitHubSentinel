import json
import requests
from openai import OpenAI  # å¯¼å…¥OpenAIåº“ç”¨äºè®¿é—®GPTæ¨¡å‹
from logger import LOG  # å¯¼å…¥æ—¥å¿—æ¨¡å—

class LLM:
    def __init__(self, config):
        """
        åˆå§‹åŒ– LLM ç±»ï¼Œæ ¹æ®é…ç½®é€‰æ‹©ä½¿ç”¨çš„æ¨¡å‹ï¼ˆOpenAI æˆ– Ollamaï¼‰ã€‚

        :param config: é…ç½®å¯¹è±¡ï¼ŒåŒ…å«æ‰€æœ‰çš„æ¨¡å‹é…ç½®å‚æ•°ã€‚
        """
        self.config = config
        self.model = config.llm_model_type.lower()  # è·å–æ¨¡å‹ç±»å‹å¹¶è½¬æ¢ä¸ºå°å†™
        print(self.model)
        if self.model == "openai":
            self.client = OpenAI()  # åˆ›å»ºOpenAIå®¢æˆ·ç«¯å®ä¾‹
        elif self.model == "ollama":
            self.api_url = config.ollama_api_url  # è®¾ç½®Ollama APIçš„URL
        else:
            LOG.error(f"ä¸æ”¯æŒçš„æ¨¡å‹ç±»å‹: {self.model}")
            raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å‹ç±»å‹: {self.model}")  # å¦‚æœæ¨¡å‹ç±»å‹ä¸æ”¯æŒï¼ŒæŠ›å‡ºé”™è¯¯

    def generate_report(self, system_prompt, user_content):
        """
        ç”ŸæˆæŠ€æœ¯æŠ¥å‘Šï¼ŒåŒ…å«è¾“å…¥éªŒè¯å’Œæç¤ºè¯ä¼˜åŒ–é€»è¾‘
        
        :param system_prompt: åŸºç¡€ç³»ç»Ÿæç¤ºè¯
        :param user_content: ç”¨æˆ·æäº¤çš„åŸå§‹å†…å®¹
        :return: ç»“æ„åŒ–æŠ€æœ¯æŠ¥å‘Š
        """
        # GitHubé¡¹ç›®è¿­ä»£ç‰¹å¾å¼ºåŒ–æ¨¡æ¿
        iteration_template = """
    [GitHubè¿­ä»£è§„èŒƒ]
    1. å¿…é¡»åŒ…å«çš„è¿­ä»£è¦ç´ ï¼š
    â–º åŠŸèƒ½å¼€å‘ï¼ˆFeature commitsï¼‰
    â–º é—®é¢˜ä¿®å¤ï¼ˆIssue fixesï¼‰
    â–º æ–‡æ¡£æ›´æ–°ï¼ˆDocs updateï¼‰
    â–º æµ‹è¯•éªŒè¯ï¼ˆTest casesï¼‰
    â–º å‘å¸ƒè¯´æ˜ï¼ˆRelease notesï¼‰

    2. æŠ€æœ¯ç‰¹å¾æ ‡æ³¨ï¼š
    âš™ï¸ å…³è”Issueç¼–å·ï¼ˆå¦‚#192ï¼‰
    âš™ï¸ ç‰ˆæœ¬è·¨åº¦æ ‡è®°ï¼ˆv1.2.0â†’v1.3.0ï¼‰
    âš™ï¸ å˜æ›´åˆ†ç±»æ ‡è¯†ï¼š[Core]/[API]/[UI]
    âš™ï¸ CI/CDç®¡é“çŠ¶æ€ï¼ˆæˆåŠŸ/å¤±è´¥ï¼‰

    3. ç¤¾åŒºäº¤äº’è¦ç´ ï¼š
    ğŸ‘¥ æ¶‰åŠè´¡çŒ®è€…æ•°é‡
    ğŸ‘¥ å…³è”è®¨è®ºå¸–ï¼ˆDiscussion linkï¼‰
    ğŸ‘¥ PRå®¡æ ¸å‘¨æœŸï¼ˆå°æ—¶æ•°ï¼‰
    ğŸ‘¥ Issueå“åº”æ—¶æ•ˆ

    [å¼‚å¸¸å¤„ç†è§„åˆ™]
    ! ç¼ºå¤±commitä¿¡æ¯æ—¶æ ‡æ³¨ã€Œè¿­ä»£è®°å½•ä¸å®Œæ•´ã€
    ! ç‰ˆæœ¬å†²çªæ—¶ä¿ç•™åŸå§‹gitè®°å½•
    ! æœªå…³é—­çš„Issueæ·»åŠ ã€Œå¾…è·Ÿè¿›ã€æ ‡è®°
    """

        optimized_prompt = f"""{system_prompt}
    {iteration_template}
    """


        messages = [
            {"role": "system", "content": optimized_prompt},
            {"role": "user", "content": user_content},
        ]

        # æ¨¡å‹åˆ†å‘é€»è¾‘
        if self.model == "openai":
            return self._generate_report_openai(messages)
        elif self.model == "ollama":
            return self._generate_report_ollama(messages)
        else:
            raise ValueError(f"Unsupported model: {self.model}")

    def _generate_report_openai(self, messages):
        """
        ä½¿ç”¨ OpenAI GPT æ¨¡å‹ç”ŸæˆæŠ¥å‘Šã€‚

        :param messages: åŒ…å«ç³»ç»Ÿæç¤ºå’Œç”¨æˆ·å†…å®¹çš„æ¶ˆæ¯åˆ—è¡¨ã€‚
        :return: ç”Ÿæˆçš„æŠ¥å‘Šå†…å®¹ã€‚
        """
        LOG.info(f"ä½¿ç”¨ OpenAI {self.config.openai_model_name} æ¨¡å‹ç”ŸæˆæŠ¥å‘Šã€‚")
        try:
            response = self.client.chat.completions.create(
                model=self.config.openai_model_name,  # ä½¿ç”¨é…ç½®ä¸­çš„OpenAIæ¨¡å‹åç§°
                messages=messages
            )
            LOG.debug("GPT å“åº”: {}", response)
            return response.choices[0].message.content  # è¿”å›ç”Ÿæˆçš„æŠ¥å‘Šå†…å®¹
        except Exception as e:
            LOG.error(f"ç”ŸæˆæŠ¥å‘Šæ—¶å‘ç”Ÿé”™è¯¯ï¼š{e}")
            raise

    def _generate_report_ollama(self, messages):
        """
        ä½¿ç”¨ Ollama LLaMA æ¨¡å‹ç”ŸæˆæŠ¥å‘Šã€‚

        :param messages: åŒ…å«ç³»ç»Ÿæç¤ºå’Œç”¨æˆ·å†…å®¹çš„æ¶ˆæ¯åˆ—è¡¨ã€‚
        :return: ç”Ÿæˆçš„æŠ¥å‘Šå†…å®¹ã€‚
        """
        LOG.info(f"ä½¿ç”¨ Ollama {self.config.ollama_model_name} æ¨¡å‹ç”ŸæˆæŠ¥å‘Šã€‚")
        try:
            payload = {
                "model": self.config.ollama_model_name,  # ä½¿ç”¨é…ç½®ä¸­çš„Ollamaæ¨¡å‹åç§°
                "messages": messages,
                "max_tokens": 4000,
                "temperature": 0.7,
                "stream": False
            }

            response = requests.post(self.api_url, json=payload)  # å‘é€POSTè¯·æ±‚åˆ°Ollama API
            response_data = response.json()

            # è°ƒè¯•è¾“å‡ºæŸ¥çœ‹å®Œæ•´çš„å“åº”ç»“æ„
            LOG.debug("Ollama å“åº”: {}", response_data)

            # ç›´æ¥ä»å“åº”æ•°æ®ä¸­è·å– content
            message_content = response_data.get("message", {}).get("content", None)
            if message_content:
                return message_content  # è¿”å›ç”Ÿæˆçš„æŠ¥å‘Šå†…å®¹
            else:
                LOG.error("æ— æ³•ä»å“åº”ä¸­æå–æŠ¥å‘Šå†…å®¹ã€‚")
                raise ValueError("Ollama API è¿”å›çš„å“åº”ç»“æ„æ— æ•ˆ")
        except Exception as e:
            LOG.error(f"ç”ŸæˆæŠ¥å‘Šæ—¶å‘ç”Ÿé”™è¯¯ï¼š{e}")
            raise

if __name__ == '__main__':
    from config import Config  # å¯¼å…¥é…ç½®ç®¡ç†ç±»
    config = Config()
    llm = LLM(config)

    markdown_content="""
# Progress for langchain-ai/langchain (2024-08-20 to 2024-08-21)

## Issues Closed in the Last 1 Days
- partners/chroma: release 0.1.3 #25599
- docs: few-shot conceptual guide #25596
- docs: update examples in api ref #25589
"""

    # ç¤ºä¾‹ï¼šç”Ÿæˆ GitHub æŠ¥å‘Š
    system_prompt = "Your specific system prompt for GitHub report generation"
    github_report = llm.generate_report(system_prompt, markdown_content)
    LOG.debug(github_report)
